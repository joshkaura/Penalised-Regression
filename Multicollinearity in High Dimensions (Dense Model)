#Monte Carlo simulations: LASSO vs Ridge for muliticollinearity (dense):

library(glmnet)
library(MASS)

# setup
set.seed(1)
rho <- c(0.0, 0.3, 0.6, 0.9) # correlation
N <- 500 # number of observations
n.sim <- 100 # number of simulations
n.x <- 100 # number of covariates
# container matrix; rows = simulations; cols = correlations 
l.mse <- matrix(NA, n.sim, length(rho)) # lasso 
r.mse <- matrix(NA, n.sim, length(rho)) # ridge
ols.mse <- matrix(NA, n.sim, length(rho)) #OLS
grid <- 10^seq(4, -2, length = 100) # 100 lambda values
l.lambda <- matrix(NA, n.sim, length(rho))
r.lambda <- matrix(NA, n.sim, length(rho))

start.time <- Sys.time()

# loop over correlations
for (j in 1:length(rho)){
  # loop over number of simulations
  for (i in 1:n.sim){
    
    # current level of correlation 
    rho.j <- rho[j]
    # variance covariance matrix filled with current correlation
    Sigma <- matrix(rho.j, nrow=n.x, ncol=n.x)
    # set the diagonal to 1
    diag(Sigma) <- 1
    # 100 (N) random draws of (n.x) covariates with mean 0 and variance sigma
    X <- MASS::mvrnorm(N, rep(0,n.x), Sigma)
    # random noise, draws from standard normal
    e <- rnorm(N)
    # true data generation process (all betas are exactly 1) => dense data generating mechanism
    y <- rowSums(X) + e
    # split into training/ test data
    train <- sample(x = c(1:N), size = N/2)
    
    ## OLS
    ols.mod <- lm(y[train]~X[train,])
    y.hatols <- predict(ols.mod, newdata = as.data.frame(X[-train, ]))
    ols.mse[i,j] <- (1/(N/2))*mean((y[-train]-y.hatols)^2)
    
    ## ridge
    # cross-validation on ridge to find best of 100 lambda values
    r.mod <- cv.glmnet(as.matrix(X[train,]), y[train], alpha = 0, lambda = grid)
    # predict outcome using the model with the best lambda
    y.hatr <- predict(r.mod, s = r.mod$lambda.min, newx = X[-train, ])
    # optimal lambda
    r.lambda[i,j] <- r.mod$lambda.min
    # MSE
    r.mse[i,j] <- mean((y[-train]-y.hatr)^2)
    
    ## lasso
    # cross-validation on ridge to find best of 100 lambda values
    l.mod <- cv.glmnet(X[train,], y[train], alpha = 1, lambda = grid)
    # predict outcome using the model with the best lambda
    y.hatl <- predict(l.mod, s = l.mod$lambda.min, newx = X[-train, ])
    # optimal lambda
    l.lambda[i,j] <- l.mod$lambda.min
    # MSE
    l.mse[i,j] <- mean((y[-train]-y.hatl)^2)
  } 
}

# how long did the MC take?
end.time <- Sys.time()
end.time-start.time

#optimal lambda values:
r.lambda
l.lambda
apply(r.lambda,2,mean) #average lambda of ridge model
apply(l.lambda,2,mean) #average lambda of lasso model


#Coefficients 
coefficients <- cbind(coef(ols.mod),
                      coef(r.mod),
                      coef(l.mod))
colnames(coefficients) <- c("OLS","Ridge","LASSO")
coefficients
t(coefficients)

#MSEs
apply(ols.mse,2,mean) #average MSE of OLS model
apply(l.mse,2,mean) #average MSE of lasso model
apply(r.mse,2,mean) #average MSE of ridge model

ols.mse

train

